# If you run this notebook in Google Colab, ensure that a GPU is enabled:
#   Runtime ▸ Change runtime type ▸ GPU

!pip install -q -U datasets transformers accelerate evaluate scikit-learn torchmetrics# Emotion Classification Model (Text)

This notebook trains a high-quality textual emotion recognition model based on a pre-trained Transformer (DistilBERT).
It is designed to run end-to-end in **Google Colab** (GPU) and produce production-ready artefacts (saved model & tokenizer).

**Pipeline**
1. Install dependencies
2. Load and explore the `emotion` dataset (≈ 20 k examples, 6 labels)
3. Tokenise texts with the `distilbert-base-uncased` tokenizer
4. Fine-tune the model with best-practice settings: mixed precision, lr-scheduler, weight decay, gradient accumulation, early stopping
5. Evaluate with *accuracy* & *macro-F1*
6. Save model + tokenizer in `emotion_model/` and demonstrate inference