import os
import random
import numpy as np
import torch
from datasets import load_dataset
from transformers import (AutoTokenizer, AutoModelForSequenceClassification, Trainer,
                          TrainingArguments, DataCollatorWithPadding, EarlyStoppingCallback)
from sklearn.metrics import accuracy_score, f1_score

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)# Install dependencies (only needed on first run)
# Remove the leading '!' if running in an environment where shell commands are not allowed.
%pip install -q -U datasets transformers accelerate evaluate scikit-learn tqdm# Emotion Detection from User Text Responses

## 1. Environment Setup
We rely on the latest stable versions of the HuggingFace ecosystem and common ML tooling.  Uncomment the `pip` cell below if running for the first time or on a fresh environment.

This notebook demonstrates how to train and deploy a high-quality text-based emotion-classification model suitable for production use.

We will fine-tune a pre-trained Transformer (DistilBERT) on the public *Emotion* dataset (6 emotional classes) using HuggingFaceâ€™s `datasets`, `transformers`, and `accelerate` libraries.

Goals:
1. Download and explore the dataset
2. Tokenise and prepare the data
3. Fine-tune `distilbert-base-uncased` with best-practices training arguments (mixed precision, gradient accumulation, learning-rate scheduling, early stopping)
4. Evaluate on a held-out test set with accuracy & macro-F1
5. Save the artefacts (model + tokenizer) for inference in mobile / server environments
6. Provide an inference helper for easy integration into Kotlin Multiplatform or any backend service

*Estimated runtime on a modern GPU < 10 minutes.*